{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giordamaug/IEEE-JBHI/blob/main/CV_emb_lgbm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import category_encoders as ce\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    matthews_corrcoef, confusion_matrix, accuracy_score, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ggithub.com/giordamaug/IEEE-JBHI/blob/main/dataset_sintetico_completo.json\n",
    "!wget https://raw.githubusercontent.com/ggithub.com/giordamaug/IEEE-JBHI/blob/main/targets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    patologies = \"0,1,2,3,4,5,6,7\"\n",
    "    input_file = \"dataset_sintetico_completo.json\"\n",
    "    target_file = \"targets.csv\"\n",
    "    splen = \"1\"\n",
    "    min_events = 3\n",
    "    display = False\n",
    "    to_latex = True\n",
    "    use_vars = \"static\"\n",
    "\n",
    "args = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "executionInfo": {
     "elapsed": 31871,
     "status": "aborted",
     "timestamp": 1747823126525,
     "user": {
      "displayName": "Maurizio Giordano",
      "userId": "11661605724852130605"
     },
     "user_tz": -120
    },
    "id": "vMWhX4V-rymy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767ab1f2c3e4459a8ec922502f6c9510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting events:   0%|          | 0/2399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_static shape: (2399, 23)\n",
      "ATTRIBUTES: ['base_pathology_area', 'bmi', 'days_after', 'days_before', 'dosage_num', 'drug', 'dyslipidemia', 'eventi_infettivi', 'gender', 'genotype_alpha1', 'genotype_alpha2', 'genotype_beta1', 'genotype_beta2', 'hbf', 'heparin', 'id', 'is_splenectomized?', 'primary_pathology', 'smoking', 'splenectomy_indication', 'splenectomy_method', 'splenectomy_response', 'tsh']\n"
     ]
    }
   ],
   "source": [
    "sequences = {}\n",
    "with open(args.input_file) as f:\n",
    "    synt_list = json.load(f)\n",
    "    for elem in tqdm(synt_list, desc=\"Extracting events\"):\n",
    "        sequences[elem['id']] = [(elem['events'][i]['event'], elem['events'][i]['date']) for i in range(len(elem['events']))]\n",
    "    sequences\n",
    "    Xstatic = pd.read_json(args.input_file).drop(columns=['events'])\n",
    "\n",
    "    print(f\"X_static shape: {Xstatic.shape}\")\n",
    "    print(f\"ATTRIBUTES: {sorted(Xstatic.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "\n",
    "class LSTMEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pooling=False):\n",
    "        super().__init__()\n",
    "        self.pooling = pooling\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hn, cn) = self.lstm(embedded)\n",
    "        if self.pooling:\n",
    "            return output.mean(dim=1)\n",
    "        else:\n",
    "            return hn.squeeze(0)\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs=10, enable_plot=False, disable=False):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        loss_history = []\n",
    "\n",
    "        self.train()\n",
    "        pbar = tqdm(range(num_epochs), disable=disable, desc=f\"Embedding:\")\n",
    "        for epoch in pbar:\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                batch = batch.long()\n",
    "                output = self(batch)\n",
    "\n",
    "                target = torch.zeros_like(output)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            pbar.set_description(f\"Embedding: Loss {avg_loss:.4f}\")\n",
    "            loss_history.append(avg_loss)\n",
    "\n",
    "            if enable_plot:\n",
    "                clear_output(wait=True)\n",
    "                plt.plot(loss_history, label=\"Loss\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                plt.title(\"Training Loss Over Time\")\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "def similarity_matrix(events, attributes, targets, emb1, emb2):\n",
    "    attributes = [a.lower() for a in attributes]\n",
    "    targets = [t.lower() for t in targets]\n",
    "    zero_data = np.ones((len(events), len(attributes)))\n",
    "    X_df = pd.DataFrame(zero_data, columns=attributes, index=events.keys())\n",
    "\n",
    "    Wmul = emb1.T@emb2\n",
    "    for id, dcount in tqdm(events.items(), desc=\"Risk calculating\"):\n",
    "        for concept in dcount.keys():\n",
    "            concept_l = concept.lower()\n",
    "            if concept_l in attributes:\n",
    "                X_df.loc[id, concept_l] += dcount[concept] * np.array([math.exp(Wmul[concept_l][disease]) for disease in targets]).mean()\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of patients by patology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "executionInfo": {
     "elapsed": 31869,
     "status": "aborted",
     "timestamp": 1747823126525,
     "user": {
      "displayName": "Maurizio Giordano",
      "userId": "11661605724852130605"
     },
     "user_tz": -120
    },
    "id": "j4xqOpJuJnJv"
   },
   "outputs": [],
   "source": [
    "# === Generazione eventi sequenze =========\n",
    "def truncevents(sequences, infection_list, max_inf=1, max_flwup=5, debug=False):\n",
    "    trunc_sequences = {}\n",
    "    # truncate event sequence to the k-th occurrence of target\n",
    "    for id in tqdm(sequences.keys(), desc=f\"Truncating to {max_inf}\"):\n",
    "        inf_cnt = 0\n",
    "        flw_cnt = 0\n",
    "        new_evset = set()\n",
    "        for e, d in sequences[id]:\n",
    "              if e in infection_list:\n",
    "                if debug: print(f\"INF[{id}] {e}\")\n",
    "                new_evset.add((e,d))\n",
    "                inf_cnt += 1\n",
    "                if inf_cnt >= max_inf: break\n",
    "              elif e == \"followup\" :\n",
    "                new_evset.add((e,d))\n",
    "                flw_cnt += 1\n",
    "                if flw_cnt >= max_flwup: break\n",
    "              else:\n",
    "                if debug: print(f\"eve[{id}] {e}\")\n",
    "                new_evset.add((e,d))\n",
    "        trunc_sequences[id] = sorted(list(new_evset), key=lambda x: x[1])\n",
    "    return trunc_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1747823126554,
     "user": {
      "displayName": "Maurizio Giordano",
      "userId": "11661605724852130605"
     },
     "user_tz": -120
    },
    "id": "-0c46KfaKoSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABOLARY SIZE: 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08dd2f28ffdd48da839eed0e8556ebab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating to 1:   0%|          | 0/2399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ST TARGET SEQUENCE: [('intervention adenoidectomy', '2020-01-01'), ('bacterial infection of the respiratory tract', '2020-01-17')]\n",
      "# INFECTED: 1030\n",
      "TARGETS: ['bacterial/viral infection of the heart', 'bacterial/viral infection of the throat', 'parasitic infection of the blood', 'fungal infection of the skin', 'bacterial infection of the gallbladder', 'bacterial infection of the biliary tract', 'autoimmune or inflammatory infection of the blood vessels', 'intestinal parasitic infection', 'viral infection of the skin and mucous membranes', 'bacterial infection of the pleural cavities', 'bacterial/viral infection of the pancreas', 'systemic viral infection', 'other infection', 'bacterial/viral infection of the respiratory tract', 'bacterial/parasitic infection of the blood', 'viral/automimetic infection of the eye', 'zoonotic bacterial infection', 'viral infection of the respiratory tract', 'bacterial/viral infection of the oral mucous membranes', 'bacterial infection of the vertebrae', 'bacterial infection of the breast', 'bacterial infection of the urogenital system', 'bacterial infection of the soft tissues', 'bacterial/viral infection of the ear', 'bacterial infection of the bones', 'bacterial/viral infection of the joints', 'localised bacterial infection', 'bacterial/viral infection of the gastrointestinal system', 'bacterial streptococcal infection', 'systemic parasitic infection', 'bacterial infection of the gastrointestinal system', 'sexually transmitted viral infection', 'respiratory viral infection', 'bacterial infection of the skin', 'systemic infection', 'bacterial infection of the respiratory tract', 'viral infection of the liver', 'bacterial/viral infection of the central nervous system', 'viral infection', 'bacterial infection of the urinary tract', 'bacterial infection of the cardiovascular system']\n",
      "# [0,1,2,3,4,5,6,7] PATIENTS: 1188\n"
     ]
    }
   ],
   "source": [
    "# Selezione pazienti\n",
    "def select_patients(df, event_counts, patologies, splen_flags=[0,1], min_ev_count=3):\n",
    "    filtered_events = {}\n",
    "    selected_patient_ids = df[\n",
    "        df['base_pathology_area'].isin(patologies) &\n",
    "        df['is_splenectomized?'].isin(splen_flags)\n",
    "    ].index.tolist()\n",
    "\n",
    "    filtered_events = {\n",
    "        int(key): value for key, value in event_counts.items()\n",
    "        if int(key) in selected_patient_ids and len(value.keys()) > min_ev_count\n",
    "    }\n",
    "\n",
    "    selected_patient_ids = np.array(list(filtered_events.keys()))\n",
    "    return selected_patient_ids, filtered_events\n",
    "\n",
    "# === Costruzione vocabolario ed eveconteggi eventi target ===\n",
    "event_counts = {}\n",
    "tot_counts = {}\n",
    "for k, v in sequences.items():\n",
    "    lista = list(map(lambda x: x[0], filter(lambda x: x[0] != ['followup'], v)))\n",
    "    flat_list = list(itertools.chain.from_iterable(lista)) if lista and isinstance(lista[0], list) else lista\n",
    "    event_counts[k] = dict((x, flat_list.count(x)) for x in set(flat_list))\n",
    "    tot_counts[int(k)] = len(flat_list)\n",
    "\n",
    "\n",
    "# === Vocabolario === \n",
    "vocab = set()\n",
    "for patient_events in sequences.values():\n",
    "    for event,_ in patient_events:\n",
    "        vocab.update([event] if isinstance(event, str) and event != 'followup' else event)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "#word_to_idx[\"followup\"] = 0\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "print(f\"VOCABOLARY SIZE: {len(vocab)}\")\n",
    "\n",
    "# Sequences indicizzate e padding (for embeddings)\n",
    "indexed_sentences = [[word_to_idx[word] if word in word_to_idx else 0\n",
    "                      for _, word in patient_events if word in word_to_idx]\n",
    "                     for patient_events in sequences.values()]\n",
    "padded_sentences = pad_sequence([torch.tensor(s) for s in indexed_sentences], batch_first=True)\n",
    "\n",
    "# Target infezioni\n",
    "#infections_terms = set(json.load(open(\"translator_infection.json\")).values())\n",
    "targets = pd.read_csv(args.target_file)['targets'].to_list()\n",
    "embed_attributes = [w for w in sorted(vocab) if w not in targets]\n",
    "\n",
    "# ===== Truncate sequences to first infection or up tofifth follow-up\n",
    "sequences = truncevents(sequences, targets)\n",
    "\n",
    "infected_ids = [id for id,l in sequences.items() if any([x[0].strip().lower() in targets for x in l]) ]\n",
    "infected_ids_last = [id for id,l in sequences.items() if len(l) > 0 and l[-1][0] in targets]\n",
    "print(f\"1ST TARGET SEQUENCE: {sequences[infected_ids[0]]}\")\n",
    "print(f\"# INFECTED: {len(infected_ids)}\")\n",
    "print(f\"TARGETS: {targets}\")\n",
    "\n",
    "# === Selezione pazienti ===\n",
    "patologies = []\n",
    "for p in args.patologies.split(','):\n",
    "    if int(p.strip()) in Xstatic['base_pathology_area'].unique():\n",
    "        patologies.append(int(p.strip()))\n",
    "    else:\n",
    "        raise Exception(f\"Primary patology {p} not in dataset!\")\n",
    "splen_flags = list(map(int, args.splen.split(',')))\n",
    "min_ev_count = args.min_events\n",
    "\n",
    "selected_patient_ids, events = select_patients(Xstatic, event_counts, patologies, splen_flags=splen_flags, min_ev_count=min_ev_count)\n",
    "print(f\"# [{args.patologies}] PATIENTS: {len(selected_patient_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbe2Ck-_rc93"
   },
   "source": [
    "## Cross validation + embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1747823126555,
     "user": {
      "displayName": "Maurizio Giordano",
      "userId": "11661605724852130605"
     },
     "user_tz": -120
    },
    "id": "y1Glx1pXrcXX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db94427504944a499eaab8e48a9edd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/1188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c945def74b46efa5663620fe3c21b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([1014,  174])) count: 0\n",
      "\n",
      "üöÄ Inizio cross-validation...\n",
      "\n",
      "X_static shape (950, 23) (238, 23) Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's MCC: 0.0527838\n",
      "X_static shape (950, 23) (238, 23) Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's MCC: 0.0516939\n",
      "X_static shape (950, 23) (238, 23) Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's MCC: 0\n",
      "X_static shape (951, 23) (237, 23) Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's MCC: 0.101251\n",
      "X_static shape (951, 23) (237, 23) Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's MCC: 0.139568\n",
      "\n",
      "üìä Risultati medi su 5 fold:\n",
      "üìà AUC:      0.5319 ¬± 0.0393\n",
      "üß™ F1-score: 0.1847 ¬± 0.1060\n",
      "‚öñÔ∏è Precision:0.1629 ¬± 0.0823\n",
      "üîÅ Recall:   0.2482 ¬± 0.1793\n",
      "üßÆ MCC:      0.0691 ¬± 0.0476\n",
      "üéØ Accuracy: 0.7482 ¬± 0.0744\n",
      "\n",
      "üß© Confusion Matrix finale (aggregata):\n",
      "[[846 168]\n",
      " [131  43]]\n",
      "0+1+2+3+4+5+6+7 & 1188 & $\\mathbf{X}^{\\text{static}}$ & $0.5319\\pm0.0393$ & $0.1847\\pm0.1060$ & $0.1629\\pm0.0823$ & $0.2482\\pm0.1793$ & $0.0691\\pm0.0476$ & [[846 168] [131  43]]\\\\ \n"
     ]
    }
   ],
   "source": [
    "# Embedding parameters\n",
    "num_epochs = 10  # 10\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64  # 16\n",
    "hidden_dim = 128\n",
    "batch_size = 32\n",
    "\n",
    "window_size = 2  # for skipgram\n",
    "\n",
    "# Bilanciamento automatico\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',\n",
    "    'verbosity': -3,\n",
    "    'is_unbalance': True\n",
    "}\n",
    "\n",
    "def mcc_eval(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_labels)\n",
    "    return 'MCC', mcc, True\n",
    "\n",
    "mcc_scores = []\n",
    "acc_scores = []\n",
    "rocauc_scores = []\n",
    "prec_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "y_valid_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# remove outliers\n",
    "if False:\n",
    "    selected_patient_ids = np.setdiff1d(selected_patient_ids, outliers_idx)\n",
    "    events = {id: events[id] for id in selected_patient_ids}\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "zero_data = np.zeros((len(events.keys()),))\n",
    "y_df = pd.DataFrame(zero_data, columns=['target']).set_index(pd.Series([e for e in list(events.keys())]))\n",
    "cnt = 0\n",
    "for id, dcount in tqdm(events.items(), desc=\"Targets\"):\n",
    "    #if disease in [e.lower() for e in dcount.keys()]:  # if there's at least one occurrence of the target set 1\n",
    "    if len(set([e.lower() for e in dcount.keys()]).intersection(set(targets))) > 0:  # if there's at least one occurrence of the target set 1\n",
    "        y_df.loc[id, 'target'] = 1\n",
    "y = y_df.values.astype(np.float32).ravel()\n",
    "print(y_df.shape)\n",
    "cvfolding = tqdm(skf.split(selected_patient_ids, y), total=n_splits, desc=\"Folds\")\n",
    "print(np.unique(y_df.values, return_counts=True), f\"count: {cnt}\")\n",
    "\n",
    "use_vars = []\n",
    "for v in args.use_vars.split(\",\"):\n",
    "    if v.strip() in [\"static\", \"binary\", \"lstm\"]:\n",
    "        use_vars += [v.strip()]\n",
    "    else:\n",
    "        raise Exception(\"Wrong methid in paramters\")\n",
    "\n",
    "print(\"\\nüöÄ Inizio cross-validation...\\n\")\n",
    "for fold, (t_idx, v_idx) in enumerate(cvfolding):\n",
    "\n",
    "    train_idx, valid_idx = selected_patient_ids[t_idx], selected_patient_ids[v_idx]\n",
    "    Xtrains = dict(zip(use_vars, [pd.DataFrame(index=train_idx)]* len(use_vars)))\n",
    "    Xtests = dict(zip(use_vars, [pd.DataFrame(index=valid_idx)]* len(use_vars)))\n",
    "    if \"lstm\" in use_vars:\n",
    "        train_sentences = [[word_to_idx[word] for word,_ in sequences[id]] for id in train_idx]\n",
    "        padded_train = pad_sequence([torch.tensor(s) for s in train_sentences], batch_first=True)\n",
    "        # Dataset e dataloader\n",
    "        embedding_dataset = TextDataset(padded_train)\n",
    "        dataloader = DataLoader(embedding_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # Inizializza e allena modello LSTM unidirezionale\n",
    "        embmodel = LSTMEmbeddingModel(len(word_to_idx), embed_dim=64, hidden_dim=128, pooling=False)\n",
    "        embmodel.train_model(dataloader, enable_plot=False, disable=False)\n",
    "        word_indices = [idx for word, idx in word_to_idx.items() if word != \"<PAD>\"]\n",
    "        word_tensors = torch.tensor(word_indices).unsqueeze(1)  # Shape (num_words, 1)\n",
    "        embedding = embmodel(word_tensors).detach().numpy()\n",
    "        W = pd.DataFrame(embedding.T, columns=[w.lower() for w in vocab])\n",
    "        X_df = similarity_matrix(events, embed_attributes, list(targets), W, W)\n",
    "        Xtrains['lstm'] = X_df.loc[train_idx]\n",
    "        Xtests['lstm'] = X_df.loc[valid_idx]\n",
    "    if \"dome\" in use_vars:\n",
    "        raise Exception(\"DOME not included in demo...\")\n",
    "    if \"static\" in use_vars:\n",
    "        Xtrains['static'] = Xstatic.loc[train_idx]\n",
    "        Xtests['static'] = Xstatic.loc[valid_idx]\n",
    "    if \"binary\" in use_vars:\n",
    "        raise Exception(\"Binary not included in demo...\")\n",
    "        #Xtrains['binary'] = Xbin[bincolumns].loc[train_idx]\n",
    "        #Xtests['binary'] = Xbin[bincolumns].loc[valid_idx]\n",
    "    if \"dummy\" in use_vars:\n",
    "        Xtrains['dummy'] = pd.DataFrame(np.random.rand(len(train_idx), 128), index=train_idx)\n",
    "        Xtests['dummy'] = pd.DataFrame(np.random.rand(len(valid_idx), 128), index=valid_idx)\n",
    "    for v in use_vars:\n",
    "        print(f\"X_{v} shape {Xtrains[v].shape} {Xtests[v].shape}\", end=' ')\n",
    "    X_df_train = pd.concat(list(Xtrains.values()), axis=1)\n",
    "    X_df_tests = pd.concat(list(Xtests.values()), axis=1)\n",
    "    X_train, X_valid = X_df_train.values.astype(np.float32), X_df_tests.values.astype(np.float32)\n",
    "    y_train, y_valid = y_df.loc[train_idx].values.astype(np.float32).ravel(), y_df.loc[valid_idx].values.astype(np.float32).ravel()\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        feval=mcc_eval,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Metriche fold\n",
    "    mcc = matthews_corrcoef(y_valid, y_pred_labels)\n",
    "    acc = accuracy_score(y_valid, y_pred_labels)\n",
    "    rocauc = roc_auc_score(y_valid, y_pred)\n",
    "    prec = precision_score(y_valid, y_pred_labels)\n",
    "    recall = recall_score(y_valid, y_pred_labels)\n",
    "    f1 = f1_score(y_valid, y_pred_labels)\n",
    "\n",
    "    # Salva\n",
    "    mcc_scores.append(mcc)\n",
    "    acc_scores.append(acc)\n",
    "    rocauc_scores.append(rocauc)\n",
    "    prec_scores.append(prec)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    y_valid_all.extend(y_valid)\n",
    "    y_pred_all.extend(y_pred_labels)\n",
    "\n",
    "    # üß† Aggiorna tqdm\n",
    "    cvfolding.set_postfix({\n",
    "        \"Fold\": fold + 1,\n",
    "        \"MCC\": f\"{mcc:.4f}\",\n",
    "        \"AUC\": f\"{rocauc:.4f}\",\n",
    "        \"Acc\": f\"{acc:.4f}\",\n",
    "        \"F1\": f\"{f1:.4f}\"\n",
    "    })\n",
    "\n",
    "# Final confusion matrix\n",
    "cm_final = confusion_matrix(y_valid_all, y_pred_all)\n",
    "\n",
    "print(f\"\\nüìä Risultati medi su {n_splits} fold:\")\n",
    "print(f\"üìà AUC:      {np.mean(rocauc_scores):.4f} ¬± {np.std(rocauc_scores):.4f}\")\n",
    "print(f\"üß™ F1-score: {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}\")\n",
    "print(f\"‚öñÔ∏è Precision:{np.mean(prec_scores):.4f} ¬± {np.std(prec_scores):.4f}\")\n",
    "print(f\"üîÅ Recall:   {np.mean(recall_scores):.4f} ¬± {np.std(recall_scores):.4f}\")\n",
    "print(f\"üßÆ MCC:      {np.mean(mcc_scores):.4f} ¬± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"üéØ Accuracy: {np.mean(acc_scores):.4f} ¬± {np.std(acc_scores):.4f}\")\n",
    "\n",
    "print(f\"\\nüß© Confusion Matrix finale (aggregata):\\n{cm_final}\")\n",
    "\n",
    "if args.display:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Pred 0', 'Pred 1'],\n",
    "                yticklabels=['True 0', 'True 1'])\n",
    "    plt.title('Final Confusion Matrix (all folds)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if args.to_latex:\n",
    "    outstr = '||'.join(['\\\\mathbf{X}^{\\\\text{'+attrtype+'}}' for attrtype in use_vars])\n",
    "    #print(f\"{'+'.join(args.patologies.split(','))}({'+'.join(['S' if s == '1' else 'N' if s == '0' else 'X' for s in args.splen.split(',')])}) & {len(selected_patient_ids)} \", end='')\n",
    "    print(f\"{'+'.join(args.patologies.split(','))} & {len(selected_patient_ids)} \", end='')\n",
    "    print(f\"& ${outstr}$ & \", end='')\n",
    "    print(f\"${np.mean(rocauc_scores):.4f}\\\\pm{np.std(rocauc_scores):.4f}$ & \", end='')\n",
    "    print(f\"${np.mean(f1_scores):.4f}\\\\pm{np.std(f1_scores):.4f}$ & \", end='')\n",
    "    print(f\"${np.mean(prec_scores):.4f}\\\\pm{np.std(prec_scores):.4f}$ & \", end='')\n",
    "    print(f\"${np.mean(recall_scores):.4f}\\\\pm{np.std(recall_scores):.4f}$ & \", end='')\n",
    "    print(f\"${np.mean(mcc_scores):.4f}\\\\pm{np.std(mcc_scores):.4f}$ & \", end='')\n",
    "    #print(f\"${np.mean(acc_scores):.4f}\\\\pm{np.std(acc_scores):.4f}$ & \", end='')\n",
    "    print(f\"{cm_final}\".replace('\\n', ''), end='')\n",
    "    print(f\"\\\\\\\\ \\n\", end='')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMo6epXy2YjZkWW4R5gqA0B",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pygeometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
